import re
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

from database import SessionLocal
from models import Festival

def clean_text(text):
    return re.sub(r"(ì•„ì´ì½˜|ì£¼ì†Œ|\s+)", " ", text).strip()

def clean_date(text):
    text = re.sub(r"\s+", "", text)
    match = re.search(r"(20[0-9]{2}[./]?\d{1,2}[./]?\d{1,2}).*?(20[0-9]{2}[./]?\d{1,2}[./]?\d{1,2})", text)
    return f"{match.group(1)} ~ {match.group(2)}" if match else text.strip()

def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def crawl_sjcf(driver):
    driver.get("https://www.sjcf.or.kr/content.do?key=2111060043")
    time.sleep(2)
    info = {}

    try:
        desc = driver.find_element(By.XPATH, "//span[contains(text(), 'ì†Œê°œ')]/following-sibling::ul")
        info["description"] = desc.text.strip()
    except: pass

    try:
        date = driver.find_element(By.XPATH, "//span[contains(text(), 'ê¸°ê°„')]/following-sibling::ul")
        info["date"] = clean_date(date.text)
    except: pass

    try:
        loc = driver.find_element(By.XPATH, "//span[contains(text(), 'ì¥ì†Œ')]").find_element(By.XPATH, "./following::li")
        content = loc.text.strip()
        if any(k in content for k in ["ì„¸ì¢…í˜¸ìˆ˜ê³µì›", "ì„¸ì¢…ì¤‘ì•™ê³µì›", "ì¼ì›", "ê´‘ì¥", "ë¬´ëŒ€", "ë„ë¡œ", "íŠ¹ë³„ìì¹˜ì‹œ"]):
            info["location"] = clean_text(content)
    except: pass

    try:
        tel = driver.find_element(By.CSS_SELECTOR, "a[href^='tel']")
        info["contact"] = tel.text.strip()
    except: pass

    try:
        img = driver.find_element(By.CSS_SELECTOR, "div.img_box img")
        src = img.get_attribute("src")
        info["image_url"] = src if src.startswith("http") else "https://www.sjcf.or.kr" + src
    except: pass

    return info

def crawl_sejong_tour(driver):
    driver.get("https://www.sejong.go.kr/tour/sub02_0101.do")
    time.sleep(2)
    info = {}

    try:
        desc = driver.find_element(By.CSS_SELECTOR, "div.info_txt > p")
        info["description"] = desc.text.strip()
    except: pass

    try:
        for li in driver.find_elements(By.CSS_SELECTOR, "ul li"):
            text = li.text.strip()
            if "ì¥ì†Œ" in text and "location" not in info:
                info["location"] = clean_text(text.replace("ì¥ì†Œ", ""))
            elif "ê¸°ê°„" in text and "date" not in info:
                info["date"] = clean_date(text.replace("ê¸°ê°„", ""))
            elif "ì—°ë½ì²˜" in text and "contact" not in info:
                info["contact"] = text.replace("ì—°ë½ì²˜", "")
    except: pass

    try:
        img = driver.find_element(By.CSS_SELECTOR, "div.img_box img")
        src = img.get_attribute("src")
        info["image_url"] = src if src.startswith("http") else "https://www.sejong.go.kr" + src
    except: pass

    return info

def crawl_sejong_festival():
    driver = get_driver()
    info = {
        "name": "ì„¸ì¢…ì¶•ì œ",
        "url": "http://sjfestival.kr/",
        "time": "í™ˆí˜ì´ì§€ ì°¸ê³ "
    }

    for data in [crawl_sjcf(driver), crawl_sejong_tour(driver)]:
        for k, v in data.items():
            if v and not info.get(k):
                info[k] = v

    info["programs"] = "http://sjfestival.kr/dh_product/prod_list"
    info["schedule"] = "http://sjfestival.kr/dh/program_schedule"

    if not info.get("description"):
        info["description"] = "ì„¸ì¢…ì¶•ì œ ì†Œê°œëŠ” í™ˆí˜ì´ì§€ ì°¸ê³ "

    driver.quit()
    return info

def save_to_db(info):
    try:
        db = SessionLocal()
        # ì´ë¦„ìœ¼ë¡œë§Œ ì¤‘ë³µ ì²´í¬ (ë” ìœ ì—°í•œ ë°©ì‹)
        exists = db.query(Festival).filter(Festival.name == info["name"]).first()

        if not exists:
            # ìƒˆ ë°ì´í„° ìƒì„±
            save_data = {
                "name": info.get("name", ""),
                "date": info.get("date", ""),
                "time": info.get("time", ""),
                "location": info.get("location", ""),
                "description": info.get("description", ""),
                "contact": info.get("contact", ""),
                "image_url": info.get("image_url", ""),
                "programs": info.get("programs", ""),
                "schedule": info.get("schedule", ""),
                "url": info.get("url", "")
            }
            new_festival = Festival(**save_data)
            db.add(new_festival)
            db.commit()
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {info['name']} (ID: {new_festival.id})")
        else:
            # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸ (ë” ë‚˜ì€ ì •ë³´ê°€ ìˆìœ¼ë©´)
            updated = False
            for key, value in info.items():
                if key != 'id' and hasattr(exists, key) and value:
                    current_value = getattr(exists, key)
                    if not current_value or (isinstance(value, str) and len(value) > len(current_value)):
                        setattr(exists, key, value)
                        updated = True
            
            if updated:
                db.commit()
                print(f"ğŸ”„ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {info['name']} (ID: {exists.id})")
            else:
                print(f"â„¹ï¸ ê¸°ì¡´ ë°ì´í„°ê°€ ë” ìš°ìˆ˜í•¨: {info['name']} (ID: {exists.id})")
        db.close()
        return True
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {info['name']} - {e}")
        import traceback
        print(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        if 'db' in locals():
            db.close()
        return False

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    info = crawl_sejong_festival()
    save_to_db(info)
